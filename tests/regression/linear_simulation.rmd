---
title: "Linear regression"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: united
    df_print: paged
params:
  run_mode: "html"
---

## Linear regression in a nutshell

Linear regression is a method to model the relationship between a continuous dependent variable \(Y\) and one or more independent variables \(X\). It estimates coefficients \(\beta\) such that:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \gamma_1 Z_1 + \gamma_2 Z_2 + \dots + \gamma_q Z_q + \epsilon
$$

Where:

* $Y$ : dependent (phenotype) variable you are trying to predict
* $X_1, X_2, \dots, X_p$ : main independent variables of interest
* $\beta_0$ : intercept (expected value of $Y$ when all predictors are zero)
* $\beta_1, \dots, \beta_p$ : coefficients for main predictors $X_1 \dots X_p$
* $Z_1, \dots, Z_q$ : covariates (additional variables you adjust for, e.g., age, sex, PCs in genetics)
* $\gamma_1, \dots, \gamma_q$ : coefficients for covariates
* $\epsilon$ : error term capturing variation in $Y$ not explained by the model

### Collinearity

Collinearity occurs when two or more predictor variables are highly correlated, meaning that in the stoat case every regression table are collinear. This can lead to:

- Inflated standard errors
- Unstable coefficient estimates
- Difficulty in interpreting the effect of individual predictors

#### Example in R:


```{r near-perfect-collinearity, echo=TRUE}
# X2 is almost identical to X1
cor(X1, X2)  # correlation near 1

# Attempt to fit model
model2 <- lm(Y ~ X1 + X2)
summary(model2)
```

### Near-perfect collinearity case

When predictors are nearly perfectly collinear, the model may fail to estimate coefficients properly (this case can happend when the number of column are > 2 and at 2 column are identical) :

```{r collinearity-example, echo=TRUE}
# Simulate correlated predictors
set.seed(123)
X1 <- rnorm(100)
X2 <- X1 + rnorm(100, sd=0.01)  # nearly collinear with X1
Y <- 5 + 2*X1 + 3*X2 + rnorm(100)

# Fit linear model
model <- lm(Y ~ X1 + X2)
summary(model)
````

### Linear Regression test

```{r import, echo=FALSE, message=FALSE}
# install.packages("RcppEigen", repos = "https://cloud.r-project.org")   # if not installed
# install.packages("RcppGSL", repos = "https://cloud.r-project.org")     # if not installed

suppressPackageStartupMessages({
library(Rcpp)
library(RcppEigen)
library(RcppGSL)
library(ggplot2)
library(reshape2)
})

Sys.setenv("CXX14FLAGS"="-std=gnu++14")
```

```{r setup, echo=FALSE, message=FALSE}
# Load C++ implementations
sourceCpp("/home/mbagarre/Bureau/stoat/tests/regression/linear_regression_maths.cpp")
sourceCpp("/home/mbagarre/Bureau/stoat/tests/regression/linear_regression_stoat.cpp")
sourceCpp("/home/mbagarre/Bureau/stoat/tests/regression/linear_regression_rvtest.cpp")

# Simulation parameters
set.seed(123)
n_sims <- 1000 # number of simulation
n <- 1000 # sample number
k <- 5 # total predictors incl. intercept [default : 2 path] + intercept
vals <- c(0.5, 1)
p_threshold <- 0.01 # pvalue threshold
intercept <- 1 # intercept value
sigma <- 1 # noise level
beta_1 <- 0.5 # beta 1 coef
beta_lin <- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2)

# Storage for simulation results
results <- data.frame(
  P_R = numeric(n_sims),
  P_Maths = numeric(n_sims),
  P_Stoat = numeric(n_sims),
  P_RVTest = numeric(n_sims),
  Diff_P_Maths = numeric(n_sims),
  Diff_P_Stoat = numeric(n_sims),
  Diff_P_RVTest = numeric(n_sims),
  N = integer(n_sims),
  K = integer(n_sims)
)

results_beta <- data.frame(
  Beta      = numeric(n_sims * length(beta_lin)),
  N         = integer(n_sims * length(beta_lin)),
  K         = integer(n_sims * length(beta_lin)),
  Coef_R    = numeric(n_sims * length(beta_lin)),
  P_R       = numeric(n_sims * length(beta_lin)),
  Coef_Maths = numeric(n_sims * length(beta_lin)),
  P_Maths   = numeric(n_sims * length(beta_lin)),
  Diff_P_Maths   = numeric(n_sims * length(beta_lin)),
  Diff_Coef_Maths = numeric(n_sims * length(beta_lin)),
  Coef_Stoat = numeric(n_sims * length(beta_lin)),
  P_Stoat   = numeric(n_sims * length(beta_lin)),
  Diff_P_Stoat   = numeric(n_sims * length(beta_lin)),
  Diff_Coef_Stoat = numeric(n_sims * length(beta_lin)),
  Coef_RVTest = numeric(n_sims * length(beta_lin)),
  P_RVTest    = numeric(n_sims * length(beta_lin)),
  Diff_P_RVTest   = numeric(n_sims * length(beta_lin)),
  Diff_Coef_RVTest = numeric(n_sims * length(beta_lin))
)
```

```{r function, echo=FALSE, message=FALSE}

# Generate one constrained path row (sum of Paths = 1)
gen_row <- function(num_path) {
  row <- rep(0, num_path)
  if (runif(1) < 0.5) {
    # 50% chance: pick one index and set it to 1
    idx <- sample(num_path, 1)
    row[idx] <- 1
  } else {
    # 50% chance: pick two distinct indices and set both to 0.5
    idxs <- sample(num_path, 2)
    row[idxs] <- 0.5
  }
  return(row)
}

# Function to count NA or non-numeric entries in a vector
count_invalid <- function(x) {
  sum(is.na(x) | !is.numeric(x))
}

# Function to check invalid counts and stop if any are > 0
check_invalid_counts <- function(...) {
  # Count all invalid values across all provided arguments
  total_invalid <- sum(sapply(list(...), count_invalid))
  if (total_invalid > 0) {
    warning("Found ", total_invalid, " NA or non-numeric values in results.")
  }
}

# Function to summarize and print percentage differences
summarize_pct_diff <- function(pct_diff, method_name = "Method") {
  if (!is.numeric(pct_diff)) stop("Input must be numeric")
  
  stats <- data.frame(
    Method = method_name,
    Min  = min(pct_diff, na.rm = TRUE),
    Mean = mean(pct_diff, na.rm = TRUE),
    Max  = max(pct_diff, na.rm = TRUE)
  )
  
  print(stats)
  invisible(stats)  # Return stats invisibly if needed
}

# Function that simulate lr for all methods
run_linear_simulation <- function(n_sims, n, k, beta_1, sigma, results,
                                  significant = TRUE, collinearity = FALSE, merge = FALSE) {

  for (i in seq_len(n_sims)) {
    
    results$N[i] <- n
    results$K[i] <- k
    
    # Step 1: Generate X matrix
    X <- matrix(
      t(replicate(n, gen_row(k - 1))),
      nrow = n,
      ncol = k - 1
    )

    if (collinearity) {
      # Step 2bis: Add 2 new zero columns
      X <- cbind(X, matrix(0, nrow = n, ncol = 2))

      # Step 3bis: Modify the last 'number_last_element' rows
      number_last_element <- sample(1:max(1, floor(0.1 * n)), 1) # number of rows to modify 1 to 10 % of the max row
      if (number_last_element > 0) {
        start_row <- n - number_last_element + 1
        # Reset the last rows to 0
        X[start_row:n, ] <- 0
        # Set last 2 columns of these rows to 0.5
        X[start_row:n, (k+1):(k+2)] <- 0.5
      }
    }

    # error case
    if (any(rowSums(X) != 1)) stop("Some rows do not sum to 1!")
    
    # Step 2: Generate Y based on significance
    beta <- rep(0, ncol(X))
    beta[1] <- if (significant) beta_1 else 0
    Y <- as.vector(X %*% beta + rnorm(n, sd = sigma))
    
    # Step 3: R Linear Regression
    df <- data.frame(Y = Y, X)
    fit_r <- lm(Y ~ ., data = df)
    r_summary <- summary(fit_r)
    coefs <- coef(r_summary)
    
    coef_r <- coef(fit_r)[2]
    p_r <- coefs[-1, "Pr(>|t|)"][1]
    
    # error case
    if (is.na(p_r)) {
      print(r_summary)
      stop("P-value is NA. Exiting.")
    }
    
    # merge similar column
    if (merge) {
      dup_cols <- duplicated(as.data.frame(t(X)))
      X <- X[, !dup_cols, drop = FALSE]
    }

    # Drop last column
    X <- X[, -ncol(X), drop = FALSE]

    # Step 4: C++ Maths
    X_maths <- X[, -ncol(X), drop = FALSE]
    res_maths <- cpp_linear_regression_maths(X_maths, Y)
    coef_maths <- unlist(res_maths$coefficients)[2]
    p_maths <- unlist(res_maths$p_values)[2]
    
    # Step 5: C++ Stoat
    res_stoat <- cpp_linear_regression_stoat(X_maths, Y)
    coef_stoat <- unlist(res_stoat$coefficients)[2]
    p_stoat <- unlist(res_stoat$p_values)[2]
    
    # Step 6: C++ RVTest
    res_rvtest <- cpp_linear_regression_rvtest(X_maths, Y)
    coef_rvtest <- unlist(res_rvtest$coefficients)[2]
    p_rvtest <- unlist(res_rvtest$p_values)[2]
    
    # Store results
    results$Coef_R[i] <- coef_r
    results$P_R[i] <- p_r
    
    results$Coef_Maths[i] <- coef_maths
    results$P_Maths[i] <- p_maths
    results$Diff_Coef_Maths[i] <- coef_maths - coef_r
    results$Diff_P_Maths[i] <- p_maths - p_r
    
    results$Coef_Stoat[i] <- coef_stoat
    results$P_Stoat[i] <- p_stoat
    results$Diff_Coef_Stoat[i] <- coef_stoat - coef_r
    results$Diff_P_Stoat[i] <- p_stoat - p_r
    
    results$Coef_RVTest[i] <- coef_rvtest
    results$P_RVTest[i] <- p_rvtest
    results$Diff_Coef_RVTest[i] <- coef_rvtest - coef_r
    results$Diff_P_RVTest[i] <- p_rvtest - p_r
  }

  return(results)
}

run_beta_simulations <- function(beta_lin, n_sims = 100, k = 5, sigma = 1, gen_row, results) {
  
  row_idx <- 1
  
  for (beta_n in beta_lin) {
    for (i in seq_len(n_sims)) {
      
      # Step 1: Generate X
      n <- 1000
      X <- matrix(t(replicate(n, gen_row(k - 1))), nrow = n, ncol = k - 1)
      if(any(rowSums(X) != 1)) stop("Some rows do not sum to 1!")
      
      # Step 2: Generate Y
      beta_true <- rep(0, ncol(X))
      beta_true[1] <- beta_n
      Y <- as.vector(X %*% beta_true + rnorm(n, sd = sigma))
      
      # Step 3: Linear regression in R
      df <- data.frame(Y = Y, X)
      fit_r <- lm(Y ~ ., data = df)
      r_summary <- summary(fit_r)
      coefs <- coef(r_summary)
      
      results$Beta[row_idx] <- beta_n
      results$N[row_idx] <- n
      results$K[row_idx] <- k
      results$Coef_R[row_idx] <- coef(fit_r)[2]
      results$P_R[row_idx] <- coefs[-1, "Pr(>|t|)"][1]
      
      if (is.na(results$P_R[row_idx])) {
        print(r_summary)
        stop("P-value is NA. Exiting.")
      }
      
      X_maths <- X[, -ncol(X), drop = FALSE]
      
      # C++ Maths
      res_maths <- cpp_linear_regression_maths(X_maths, Y)
      results$Coef_Maths[row_idx] <- unlist(res_maths$coefficients)[2]
      results$P_Maths[row_idx] <- unlist(res_maths$p_values)[2]
      results$Diff_P_Maths[row_idx] <- results$P_Maths[row_idx] - results$P_R[row_idx]
      results$Diff_Coef_Maths[row_idx] <- results$Coef_Maths[row_idx] - results$Coef_R[row_idx]
      
      # C++ Stoat
      res_stoat <- cpp_linear_regression_stoat(X_maths, Y)
      results$Coef_Stoat[row_idx] <- unlist(res_stoat$coefficients)[2]
      results$P_Stoat[row_idx] <- unlist(res_stoat$p_values)[2]
      results$Diff_P_Stoat[row_idx] <- results$P_Stoat[row_idx] - results$P_R[row_idx]
      results$Diff_Coef_Stoat[row_idx] <- results$Coef_Stoat[row_idx] - results$Coef_R[row_idx]
      
      # C++ RVTest
      res_rvtest <- cpp_linear_regression_rvtest(X_maths, Y)
      results$Coef_RVTest[row_idx] <- unlist(res_rvtest$coefficients)[2]
      results$P_RVTest[row_idx] <- unlist(res_rvtest$p_values)[2]
      results$Diff_P_RVTest[row_idx] <- results$P_RVTest[row_idx] - results$P_R[row_idx]
      results$Diff_Coef_RVTest[row_idx] <- results$Coef_RVTest[row_idx] - results$Coef_R[row_idx]
      
      row_idx <- row_idx + 1
    }
  }
  
  return(results)
}

# Plots function
plot_linear_simulation_results <- function(results, null = FALSE, title_prefix = "") {
  
  plots <- list()
  
  # ----- Check validity -----
  check_invalid_counts(
      results$Coef_R, results$P_R,
      results$Coef_Maths, results$P_Maths,
      results$Coef_Stoat, results$P_Stoat,
      results$Coef_RVTest, results$P_RVTest,
    )
  
  cat("P-value distribution [min/max/mean] : \n")

  P_stats <- data.frame(
    Method = c("PR"),
    Min = min(results$P_R, na.rm = TRUE),
    Mean = mean(results$P_R, na.rm = TRUE),
    Max = max(results$P_R, na.rm = TRUE)
  )
  print(P_stats)

  # ----- P-value distributions -----
  df_pvals_long <- melt(
    data.frame(
      P_R      = results$P_R,
      P_Maths  = results$P_Maths,
      P_Stoat  = results$P_Stoat,
      P_RVTest = results$P_RVTest
    ),
    variable.name = "Method",
    value.name = "PValue"
  )
  
  plots$pval_hist <- ggplot(df_pvals_long, aes(x = PValue, fill = Method)) +
    geom_histogram(alpha = 0.5, position = "dodge", bins = 50) +
    theme_bw() +
    labs(title = paste(title_prefix, "P-value Distributions by Method"),
         x = "P-value", y = "Frequency")
  
  plots$pval_box <- ggplot(df_pvals_long, aes(x = Method, y = PValue, fill = Method)) +
    geom_boxplot(outlier.size = 1) +
    theme_bw() +
    labs(title = paste(title_prefix, "Distribution of P-values by Method"),
         x = "Method", y = "P-value") +
    theme(legend.position = "none")
  
  plots$pval_ecdf <- ggplot(df_pvals_long, aes(x = PValue, color = Method)) +
    stat_ecdf(linewidth = 1) +
    theme_bw() +
    labs(
      title = paste(title_prefix, "Empirical CDF of P-values by Method"),
      x = "P-value",
      y = "ECDF"
    ) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray")

  if (!null) {
    # Filtered P-values (0 to 0.0001)
    df_pvals_long_small <- subset(df_pvals_long, PValue <= 0.0001)
    plots$pval_hist_small <- ggplot(df_pvals_long_small, aes(x = PValue, fill = Method)) +
      geom_histogram(position = "dodge", bins = 30) +
      theme_bw() +
      labs(title = paste(title_prefix, "P-value Distributions (0 to 0.0001)"),
          x = "P-value", y = "Frequency")
    
    # ----- P-value differences vs R -----
    df_pvalue_diff_pvalue_long <- melt(
      data.frame(
        P_R = results$P_R,
        Maths  = results$Diff_P_Maths,
        Stoat  = results$Diff_P_Stoat,
        RVTest = results$Diff_P_RVTest
      ),
      id.vars = "P_R",
      variable.name = "Method",
      value.name = "Diff_P"
    )
  }

  plots$pval_diff_vs_R <- ggplot(df_pvalue_diff_pvalue_long,
                                 aes(x = P_R, y = Diff_P, color = Method)) +
    geom_point(alpha = 0.6) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
    theme_bw(base_size = 14) +
    labs(title = paste(title_prefix, "Difference in P-values vs R"),
         x = "P-value (R lm)",
         y = "Difference in P-value (Method - R)",
         color = "Method")
  
  # ----- P-value differences hist -----
  df_pdiff_long <- melt(
    data.frame(
      Diff_P_Maths  = results$Diff_P_Maths,
      Diff_P_Stoat  = results$Diff_P_Stoat,
      Diff_P_RVTest = results$Diff_P_RVTest
    ),
    variable.name = "Method",
    value.name = "PValueDiff"
  )
  
  plots$pval_diff_hist <- ggplot(df_pdiff_long, aes(x = PValueDiff, fill = Method)) +
    geom_histogram(bins = 50, alpha = 0.5, position = "dodge") +
    theme_bw() +
    labs(title = paste(title_prefix, "Difference in P-values vs R lm"),
         x = "C++ P-value - R P-value", y = "Frequency")
  
  # ----- Coefficient distributions -----
  df_coef_long <- melt(
    data.frame(
      Coef_R      = results$Coef_R,
      Coef_Maths  = results$Coef_Maths,
      Coef_Stoat  = results$Coef_Stoat,
      Coef_RVTest = results$Coef_RVTest
    ),
    variable.name = "Method",
    value.name = "Coefficient"
  )
  
  plots$coef_hist <- ggplot(df_coef_long, aes(x = Coefficient, fill = Method)) +
    geom_histogram(bins = 50, alpha = 0.5, position = "dodge") +
    theme_bw() +
    labs(title = paste(title_prefix, "Coefficient Distributions"),
         x = "Coefficient", y = "Frequency")
  
  # ----- Coefficient differences -----
  df_diff_long <- melt(
    data.frame(
      Diff_Coef_Maths  = results$Diff_Coef_Maths,
      Diff_Coef_Stoat  = results$Diff_Coef_Stoat,
      Diff_Coef_RVTest = results$Diff_Coef_RVTest
    ),
    variable.name = "Method",
    value.name = "CoefDiff"
  )
  
  plots$coef_diff_hist <- ggplot(df_diff_long, aes(x = CoefDiff, fill = Method)) +
    geom_histogram(bins = 50, alpha = 0.5, position = "dodge") +
    theme_bw() +
    labs(title = paste(title_prefix, "Difference in Coefficients vs R lm"),
         x = "C++ Coefficient - R Coefficient", y = "Frequency")
  
  # ---- SIGNIFICANCE proportion ----
  p_threshold <- 0.01
  sig_props <- sapply(results[, c("P_R", "P_Maths", "P_Stoat", "P_RVTest")],
                      function(p) mean(p < p_threshold, na.rm = TRUE) * 100)

  cat("% of p-value significant per method:\n")
  print(sig_props)

  # ---- Difference p-value proportion ----
  cat("Difference in P-values per method [min/max/mean %] [ALL significant]:\n")
  
  results$PctDiff_Maths  <- 100 * abs(results$P_Maths  - results$P_R) / results$P_R
  results$PctDiff_Stoat  <- 100 * abs(results$P_Stoat  - results$P_R) / results$P_R
  results$PctDiff_RVTest <- 100 * abs(results$P_RVTest - results$P_R) / results$P_R
  
  summarize_pct_diff <- function(x, name) {
    cat(paste0(name, " : min = ", round(min(x, na.rm = TRUE), 4),
               ", mean = ", round(mean(x, na.rm = TRUE), 4),
               ", max = ", round(max(x, na.rm = TRUE), 4), "\n"))
  }
  
  summarize_pct_diff(results$PctDiff_Maths, "Maths")
  summarize_pct_diff(results$PctDiff_Stoat, "Stoat")
  summarize_pct_diff(results$PctDiff_RVTest, "RVTest")

  return(plots)
}
```

# ALL VARIANT PLOT SIGNIFICANCE

```{r plot1, echo=TRUE, message=FALSE}
results_all_significatif <- run_linear_simulation(n_sims, n, k, beta_1, sigma, results, significant=TRUE, collinearity=FALSE, merge=FALSE)
plots_all_significatif <- plot_linear_simulation_results(results_all_significatif, null = FALSE, title_prefix = "[All variant type + Significant]")
plots_all_significatif$pval_hist
```

# ALL VARIANT PLOT NO SIGNIFICANCE

```{r plot2, echo=TRUE, message=FALSE}
results_all_no_significatif <- run_linear_simulation(n_sims, n, k, beta_1, sigma, results, significant=FALSE, collinearity=FALSE, merge=FALSE)
plots_all_no_sig <- plot_linear_simulation_results(results_all_no_significatif, null = TRUE, title_prefix = "[All variant type + No Significant]")
plots_all_no_sig$pval_hist
```

# NEAR PERFECT COLLINEARITY WITHOUT MERGE SAME COLUMN SIGNIFICATIF

```{r plot3, echo=TRUE, message=FALSE}
results_colinearity_significatif <- run_linear_simulation(n_sims, n, k, beta_1, sigma, results, significant=TRUE, collinearity=TRUE, merge=FALSE)
plots_colinearity_significatif <- plot_linear_simulation_results(results_colinearity_significatif, null = TRUE, title_prefix = "[All variant type + No Significant]")
plots_colinearity_significatif$pval_hist
```

# NEAR PERFECT COLLINEARITY WITHOUT MERGE SAME COLUMN NO SIGNIFICATIF

```{r plot4, echo=TRUE, message=FALSE}
results_colinearity_no_significatif <- run_linear_simulation(n_sims, n, k, beta_1, sigma, results, significant=FALSE, collinearity=TRUE, merge=FALSE)
plots_colinearity_no_significatif <- plot_linear_simulation_results(results_colinearity_no_significatif, null = TRUE, title_prefix = "[All variant type + No Significant]")
plots_colinearity_no_significatif$pval_hist
```

# NEAR PERFECT COLLINEARITY WITH MERGE SAME COLUMN SIGNIFICATIF

```{r plot5, echo=TRUE, message=FALSE}
results_colinearity_significatif_merge <- run_linear_simulation(n_sims, n, k, beta_1, sigma, results, significant=TRUE, collinearity=TRUE, merge=TRUE)
plots_colinearity_significatif_merge <- plot_linear_simulation_results(results_colinearity_significatif_merge, null = TRUE, title_prefix = "[All variant type + No Significant]")
plots_colinearity_significatif_merge$pval_hist
```

# NEAR PERFECT COLLINEARITY WITH MERGE SAME COLUMN NO SIGNIFICATIF

```{r plot6, echo=TRUE, message=FALSE}
results_colinearity_no_significatif_merge <- run_linear_simulation(n_sims, n, k, beta_1, sigma, results, significant=FALSE, collinearity=TRUE, merge=TRUE)
plots_colinearity_no_significatif_merge <- plot_linear_simulation_results(results_colinearity_no_significatif_merge, null = TRUE, title_prefix = "[All variant type + No Significant]")
plots_colinearity_no_significatif_merge$pval_hist
```

# BETA SIMULATION

```{r plot7, echo=TRUE, message=FALSE}
results_beta_simulations <- run_beta_simulations(beta_lin, n_sims, k, sigma, gen_row, results)

df_pvals_long <- melt(
  results_beta_simulations,
  id.vars = c("Beta", "N", "K"),
  measure.vars = c("P_R", "P_Maths", "P_Stoat", "P_RVTest"),
  variable.name = "Method",
  value.name = "PValue"
)

ggplot(df_pvals_long, aes(x = Beta, y = PValue, color = Method)) +
  stat_summary(fun = mean, geom = "line", linewidth = 1.2) +      # mean P-value per beta
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.1) +  # error bars
  scale_y_continuous(trans = "log10") +  # log-scale for p-values
  theme_bw() +
  labs(
    title = "Impact of Beta on P-values by Method",
    x = expression(beta),
    y = "P-value (log10 scale)"
  ) +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "gray") +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.title = element_text(size = 14)
  )
```